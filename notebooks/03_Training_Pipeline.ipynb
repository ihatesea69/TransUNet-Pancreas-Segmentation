{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 03: Training Pipeline\n",
        "\n",
        "## Pancreas CT Segmentation using TransUNet\n",
        "\n",
        "This notebook covers:\n",
        "1. Dataset preparation with SlicingDataset\n",
        "2. Hybrid Loss function (Dice + CrossEntropy)\n",
        "3. Training configuration\n",
        "4. Training loop with validation\n",
        "5. Checkpointing and logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "import monai\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.losses import DiceLoss\n",
        "from monai.data import decollate_batch\n",
        "from monai.transforms import AsDiscrete\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, str(Path.cwd()))\n",
        "\n",
        "# Import custom modules\n",
        "from src.model import create_transunet\n",
        "from src.dataset import SlicingDataset\n",
        "from src.transforms import get_train_transforms, get_val_transforms\n",
        "from src.loss import HybridLoss\n",
        "from src.utils import plot_training_history\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "CONFIG = {\n",
        "    # Data\n",
        "    \"data_dir\": Path.cwd() / \"data\" / \"Task07_Pancreas\",\n",
        "    \"splits_path\": Path.cwd() / \"outputs\" / \"data_splits.json\",\n",
        "    \"checkpoint_dir\": Path.cwd() / \"checkpoints\",\n",
        "    \n",
        "    # Model\n",
        "    \"img_size\": 224,\n",
        "    \"in_channels\": 1,\n",
        "    \"out_channels\": 2,  # Background + Pancreas (merge tumor with pancreas)\n",
        "    \"model_variant\": \"small\",  # Use small for faster training, base for best results\n",
        "    \n",
        "    # Training\n",
        "    \"batch_size\": 8,\n",
        "    \"num_epochs\": 50,\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"weight_decay\": 1e-5,\n",
        "    \n",
        "    # Loss\n",
        "    \"dice_weight\": 0.5,\n",
        "    \"ce_weight\": 0.5,\n",
        "    \n",
        "    # Data loading\n",
        "    \"num_workers\": 0,  # Set to 0 for Windows compatibility\n",
        "    \"cache_data\": False,  # Set True if you have enough RAM\n",
        "    \n",
        "    # Misc\n",
        "    \"seed\": 42,\n",
        "    \"val_interval\": 1,  # Validate every N epochs\n",
        "    \"save_interval\": 5,  # Save checkpoint every N epochs\n",
        "}\n",
        "\n",
        "# Create checkpoint directory\n",
        "CONFIG[\"checkpoint_dir\"].mkdir(exist_ok=True)\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(CONFIG[\"seed\"])\n",
        "np.random.seed(CONFIG[\"seed\"])\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data splits\n",
        "with open(CONFIG[\"splits_path\"], \"r\") as f:\n",
        "    data_splits = json.load(f)\n",
        "\n",
        "print(f\"Train samples: {len(data_splits['train'])}\")\n",
        "print(f\"Val samples: {len(data_splits['val'])}\")\n",
        "print(f\"Test samples: {len(data_splits['test'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Limit samples for faster testing (remove in production)\n",
        "MAX_TRAIN_SAMPLES = 20  # Use None for all samples\n",
        "MAX_VAL_SAMPLES = 5\n",
        "\n",
        "if MAX_TRAIN_SAMPLES:\n",
        "    train_data = data_splits[\"train\"][:MAX_TRAIN_SAMPLES]\n",
        "    val_data = data_splits[\"val\"][:MAX_VAL_SAMPLES]\n",
        "    print(f\"\\nUsing limited samples for testing:\")\n",
        "    print(f\"  Train: {len(train_data)}\")\n",
        "    print(f\"  Val: {len(val_data)}\")\n",
        "else:\n",
        "    train_data = data_splits[\"train\"]\n",
        "    val_data = data_splits[\"val\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create Datasets and DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get transforms\n",
        "train_transforms = get_train_transforms()\n",
        "val_transforms = get_val_transforms()\n",
        "\n",
        "print(\"Creating datasets...\")\n",
        "print(\"This may take a while as volumes are being indexed...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create SlicingDataset for training\n",
        "train_dataset = SlicingDataset(\n",
        "    data_list=train_data,\n",
        "    transform=train_transforms,\n",
        "    slice_axis=0,  # Axial slices\n",
        "    include_empty=False,  # Only slices with pancreas\n",
        "    cache_data=CONFIG[\"cache_data\"],\n",
        "    target_size=(CONFIG[\"img_size\"], CONFIG[\"img_size\"]),\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain dataset: {len(train_dataset)} slices\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create SlicingDataset for validation\n",
        "val_dataset = SlicingDataset(\n",
        "    data_list=val_data,\n",
        "    transform=val_transforms,\n",
        "    slice_axis=0,\n",
        "    include_empty=False,\n",
        "    cache_data=CONFIG[\"cache_data\"],\n",
        "    target_size=(CONFIG[\"img_size\"], CONFIG[\"img_size\"]),\n",
        ")\n",
        "\n",
        "print(f\"Val dataset: {len(val_dataset)} slices\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=CONFIG[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    num_workers=CONFIG[\"num_workers\"],\n",
        "    pin_memory=True if torch.cuda.is_available() else False,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=CONFIG[\"batch_size\"],\n",
        "    shuffle=False,\n",
        "    num_workers=CONFIG[\"num_workers\"],\n",
        "    pin_memory=True if torch.cuda.is_available() else False,\n",
        ")\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize a batch\n",
        "sample_images, sample_labels = next(iter(train_loader))\n",
        "print(f\"\\nBatch shapes:\")\n",
        "print(f\"  Images: {sample_images.shape}\")\n",
        "print(f\"  Labels: {sample_labels.shape}\")\n",
        "\n",
        "# Show samples\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "for i in range(4):\n",
        "    axes[0, i].imshow(sample_images[i, 0].numpy(), cmap=\"gray\")\n",
        "    axes[0, i].set_title(f\"Image {i+1}\")\n",
        "    axes[0, i].axis(\"off\")\n",
        "    \n",
        "    axes[1, i].imshow(sample_labels[i].numpy(), cmap=\"jet\", vmin=0, vmax=2)\n",
        "    axes[1, i].set_title(f\"Label {i+1}\")\n",
        "    axes[1, i].axis(\"off\")\n",
        "\n",
        "plt.suptitle(\"Training Batch Sample\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create Model, Loss, Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model\n",
        "model = create_transunet(\n",
        "    img_size=CONFIG[\"img_size\"],\n",
        "    in_channels=CONFIG[\"in_channels\"],\n",
        "    out_channels=CONFIG[\"out_channels\"],\n",
        "    variant=CONFIG[\"model_variant\"],\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create loss function\n",
        "criterion = HybridLoss(\n",
        "    dice_weight=CONFIG[\"dice_weight\"],\n",
        "    ce_weight=CONFIG[\"ce_weight\"],\n",
        "    num_classes=CONFIG[\"out_channels\"],\n",
        ")\n",
        "\n",
        "print(f\"Loss function: HybridLoss (Dice={CONFIG['dice_weight']}, CE={CONFIG['ce_weight']})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create optimizer\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=CONFIG[\"learning_rate\"],\n",
        "    weight_decay=CONFIG[\"weight_decay\"],\n",
        ")\n",
        "\n",
        "# Create learning rate scheduler\n",
        "scheduler = CosineAnnealingLR(\n",
        "    optimizer,\n",
        "    T_max=CONFIG[\"num_epochs\"],\n",
        "    eta_min=1e-6,\n",
        ")\n",
        "\n",
        "print(f\"Optimizer: AdamW (lr={CONFIG['learning_rate']}, weight_decay={CONFIG['weight_decay']})\")\n",
        "print(f\"Scheduler: CosineAnnealingLR (T_max={CONFIG['num_epochs']})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create metrics\n",
        "dice_metric = DiceMetric(\n",
        "    include_background=False,  # Exclude background from metric\n",
        "    reduction=\"mean\",\n",
        ")\n",
        "\n",
        "# Post-processing transforms\n",
        "post_pred = AsDiscrete(argmax=True, to_onehot=CONFIG[\"out_channels\"])\n",
        "post_label = AsDiscrete(to_onehot=CONFIG[\"out_channels\"])\n",
        "\n",
        "print(\"Metric: DiceMetric (excluding background)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
        "    \"\"\"\n",
        "    Train for one epoch.\n",
        "    \n",
        "    Returns:\n",
        "        Average training loss\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    pbar = tqdm(loader, desc=\"Training\", leave=False)\n",
        "    for images, labels in pbar:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "        \n",
        "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "    \n",
        "    return total_loss / num_batches\n",
        "\n",
        "\n",
        "def validate(model, loader, criterion, dice_metric, post_pred, post_label, device):\n",
        "    \"\"\"\n",
        "    Validate the model.\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (average loss, dice score)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    dice_metric.reset()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(loader, desc=\"Validation\", leave=False)\n",
        "        for images, labels in pbar:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "            \n",
        "            # Compute Dice score\n",
        "            outputs_list = decollate_batch(outputs)\n",
        "            labels_list = decollate_batch(labels.unsqueeze(1))\n",
        "            \n",
        "            outputs_post = [post_pred(o) for o in outputs_list]\n",
        "            labels_post = [post_label(l) for l in labels_list]\n",
        "            \n",
        "            dice_metric(y_pred=outputs_post, y=labels_post)\n",
        "    \n",
        "    avg_loss = total_loss / num_batches\n",
        "    dice_score = dice_metric.aggregate().item()\n",
        "    \n",
        "    return avg_loss, dice_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training history\n",
        "history = {\n",
        "    \"train_loss\": [],\n",
        "    \"val_loss\": [],\n",
        "    \"val_dice\": [],\n",
        "    \"lr\": [],\n",
        "}\n",
        "\n",
        "best_dice = 0.0\n",
        "best_epoch = 0\n",
        "\n",
        "print(f\"\\nStarting training for {CONFIG['num_epochs']} epochs...\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main training loop\n",
        "for epoch in range(1, CONFIG[\"num_epochs\"] + 1):\n",
        "    epoch_start = time.time()\n",
        "    \n",
        "    # Training\n",
        "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    history[\"train_loss\"].append(train_loss)\n",
        "    \n",
        "    # Validation\n",
        "    if epoch % CONFIG[\"val_interval\"] == 0:\n",
        "        val_loss, val_dice = validate(\n",
        "            model, val_loader, criterion, dice_metric,\n",
        "            post_pred, post_label, device\n",
        "        )\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_dice\"].append(val_dice)\n",
        "    else:\n",
        "        val_loss = history[\"val_loss\"][-1] if history[\"val_loss\"] else 0\n",
        "        val_dice = history[\"val_dice\"][-1] if history[\"val_dice\"] else 0\n",
        "    \n",
        "    # Learning rate\n",
        "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "    history[\"lr\"].append(current_lr)\n",
        "    scheduler.step()\n",
        "    \n",
        "    # Checkpoint if best\n",
        "    if val_dice > best_dice:\n",
        "        best_dice = val_dice\n",
        "        best_epoch = epoch\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "            \"best_dice\": best_dice,\n",
        "            \"config\": CONFIG,\n",
        "        }, CONFIG[\"checkpoint_dir\"] / \"best_metric_model.pth\")\n",
        "        save_msg = \" [BEST - Saved]\"\n",
        "    else:\n",
        "        save_msg = \"\"\n",
        "    \n",
        "    # Periodic checkpoint\n",
        "    if epoch % CONFIG[\"save_interval\"] == 0:\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "            \"history\": history,\n",
        "        }, CONFIG[\"checkpoint_dir\"] / f\"checkpoint_epoch_{epoch}.pth\")\n",
        "    \n",
        "    epoch_time = time.time() - epoch_start\n",
        "    \n",
        "    # Print progress\n",
        "    print(\n",
        "        f\"Epoch {epoch:3d}/{CONFIG['num_epochs']} | \"\n",
        "        f\"Train Loss: {train_loss:.4f} | \"\n",
        "        f\"Val Loss: {val_loss:.4f} | \"\n",
        "        f\"Val Dice: {val_dice:.4f} | \"\n",
        "        f\"LR: {current_lr:.2e} | \"\n",
        "        f\"Time: {epoch_time:.1f}s{save_msg}\"\n",
        "    )\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"Training complete!\")\n",
        "print(f\"Best Dice Score: {best_dice:.4f} at epoch {best_epoch}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "# Loss curves\n",
        "axes[0].plot(epochs, history[\"train_loss\"], \"b-\", label=\"Train Loss\", linewidth=2)\n",
        "if history[\"val_loss\"]:\n",
        "    val_epochs = range(CONFIG[\"val_interval\"], len(history[\"train_loss\"]) + 1, CONFIG[\"val_interval\"])\n",
        "    axes[0].plot(val_epochs, history[\"val_loss\"], \"r-\", label=\"Val Loss\", linewidth=2)\n",
        "axes[0].set_xlabel(\"Epoch\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[0].set_title(\"Training and Validation Loss\")\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Dice score curve\n",
        "if history[\"val_dice\"]:\n",
        "    val_epochs = range(CONFIG[\"val_interval\"], len(history[\"train_loss\"]) + 1, CONFIG[\"val_interval\"])\n",
        "    axes[1].plot(val_epochs, history[\"val_dice\"], \"g-\", label=\"Val Dice\", linewidth=2)\n",
        "    axes[1].axhline(y=best_dice, color=\"r\", linestyle=\"--\", label=f\"Best: {best_dice:.4f}\")\n",
        "axes[1].set_xlabel(\"Epoch\")\n",
        "axes[1].set_ylabel(\"Dice Score\")\n",
        "axes[1].set_title(\"Validation Dice Score\")\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Learning rate curve\n",
        "axes[2].plot(epochs, history[\"lr\"], \"purple\", linewidth=2)\n",
        "axes[2].set_xlabel(\"Epoch\")\n",
        "axes[2].set_ylabel(\"Learning Rate\")\n",
        "axes[2].set_title(\"Learning Rate Schedule\")\n",
        "axes[2].set_yscale(\"log\")\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(CONFIG[\"checkpoint_dir\"] / \"training_curves.png\", dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save training history\n",
        "history_path = CONFIG[\"checkpoint_dir\"] / \"training_history.json\"\n",
        "with open(history_path, \"w\") as f:\n",
        "    json.dump(history, f, indent=2)\n",
        "print(f\"Training history saved to: {history_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Quick Inference Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model\n",
        "checkpoint = torch.load(CONFIG[\"checkpoint_dir\"] / \"best_metric_model.pth\")\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "model.eval()\n",
        "\n",
        "print(f\"Loaded best model from epoch {checkpoint['epoch']} with Dice={checkpoint['best_dice']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get sample predictions\n",
        "sample_images, sample_labels = next(iter(val_loader))\n",
        "sample_images = sample_images.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions = model(sample_images)\n",
        "    predictions = torch.argmax(predictions, dim=1)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
        "\n",
        "for i in range(4):\n",
        "    # Original image\n",
        "    axes[0, i].imshow(sample_images[i, 0].cpu().numpy(), cmap=\"gray\")\n",
        "    axes[0, i].set_title(\"Input CT\")\n",
        "    axes[0, i].axis(\"off\")\n",
        "    \n",
        "    # Ground truth\n",
        "    axes[1, i].imshow(sample_images[i, 0].cpu().numpy(), cmap=\"gray\")\n",
        "    gt_mask = np.ma.masked_where(sample_labels[i].numpy() == 0, sample_labels[i].numpy())\n",
        "    axes[1, i].imshow(gt_mask, cmap=\"Greens\", alpha=0.6, vmin=0, vmax=2)\n",
        "    axes[1, i].set_title(\"Ground Truth\")\n",
        "    axes[1, i].axis(\"off\")\n",
        "    \n",
        "    # Prediction\n",
        "    axes[2, i].imshow(sample_images[i, 0].cpu().numpy(), cmap=\"gray\")\n",
        "    pred_mask = np.ma.masked_where(predictions[i].cpu().numpy() == 0, predictions[i].cpu().numpy())\n",
        "    axes[2, i].imshow(pred_mask, cmap=\"Reds\", alpha=0.6, vmin=0, vmax=2)\n",
        "    axes[2, i].set_title(\"Prediction\")\n",
        "    axes[2, i].axis(\"off\")\n",
        "\n",
        "plt.suptitle(\"Model Predictions on Validation Set\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig(CONFIG[\"checkpoint_dir\"] / \"sample_predictions.png\", dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook implemented the complete training pipeline:\n",
        "\n",
        "1. **SlicingDataset**: Extracts 2D axial slices from 3D CT volumes, filtering to include only slices with pancreas labels\n",
        "\n",
        "2. **HybridLoss**: Combines Dice Loss and Cross-Entropy Loss with configurable weights (0.5 each by default)\n",
        "\n",
        "3. **Training Configuration**:\n",
        "   - Optimizer: AdamW with learning rate 1e-4\n",
        "   - Scheduler: CosineAnnealingLR\n",
        "   - Metric: DiceMetric (excluding background)\n",
        "\n",
        "4. **Training Loop**:\n",
        "   - Forward pass -> Loss computation -> Backward pass -> Optimizer step\n",
        "   - Validation with Dice score computation\n",
        "   - Best model checkpointing\n",
        "\n",
        "5. **Outputs**:\n",
        "   - `checkpoints/best_metric_model.pth`: Best model weights\n",
        "   - `checkpoints/training_history.json`: Loss and metric history\n",
        "   - `checkpoints/training_curves.png`: Training visualization\n",
        "\n",
        "Next: Notebook 04 - Evaluation and Demo"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
