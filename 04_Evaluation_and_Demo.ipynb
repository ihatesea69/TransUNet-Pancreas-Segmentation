{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 04: Evaluation and Demo\n",
        "\n",
        "## Pancreas CT Segmentation using TransUNet\n",
        "\n",
        "This notebook covers:\n",
        "1. Loading trained model\n",
        "2. 3D volume inference pipeline\n",
        "3. Metric computation (Dice, Hausdorff Distance)\n",
        "4. Publication-ready visualizations\n",
        "5. Attention map visualization (Bonus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import nibabel as nib\n",
        "from tqdm import tqdm\n",
        "from scipy.ndimage import zoom\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import monai\n",
        "from monai.metrics import DiceMetric, HausdorffDistanceMetric\n",
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    LoadImaged,\n",
        "    EnsureChannelFirstd,\n",
        "    Orientationd,\n",
        "    Spacingd,\n",
        "    ScaleIntensityRanged,\n",
        "    CropForegroundd,\n",
        "    EnsureTyped,\n",
        ")\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, str(Path.cwd()))\n",
        "\n",
        "# Import custom modules\n",
        "from src.model import create_transunet\n",
        "from src.transforms import get_val_transforms\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "CHECKPOINT_PATH = PROJECT_ROOT / \"checkpoints\" / \"best_metric_model.pth\"\n",
        "SPLITS_PATH = PROJECT_ROOT / \"outputs\" / \"data_splits.json\"\n",
        "OUTPUT_DIR = PROJECT_ROOT / \"outputs\"\n",
        "\n",
        "# Configuration\n",
        "IMG_SIZE = 224\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "print(f\"Checkpoint: {CHECKPOINT_PATH}\")\n",
        "print(f\"Splits: {SPLITS_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load checkpoint\n",
        "if CHECKPOINT_PATH.exists():\n",
        "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
        "    print(f\"Loaded checkpoint from epoch {checkpoint['epoch']}\")\n",
        "    print(f\"Best validation Dice: {checkpoint['best_dice']:.4f}\")\n",
        "    \n",
        "    # Get config from checkpoint\n",
        "    config = checkpoint.get('config', {})\n",
        "    model_variant = config.get('model_variant', 'small')\n",
        "else:\n",
        "    print(\"Warning: No checkpoint found. Using default model.\")\n",
        "    checkpoint = None\n",
        "    model_variant = 'small'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model\n",
        "model = create_transunet(\n",
        "    img_size=IMG_SIZE,\n",
        "    in_channels=1,\n",
        "    out_channels=NUM_CLASSES,\n",
        "    variant=model_variant,\n",
        ")\n",
        "\n",
        "# Load weights\n",
        "if checkpoint:\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(\"Model weights loaded successfully.\")\n",
        "\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(f\"\\nModel variant: {model_variant}\")\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data splits\n",
        "with open(SPLITS_PATH, \"r\") as f:\n",
        "    data_splits = json.load(f)\n",
        "\n",
        "test_data = data_splits[\"test\"]\n",
        "print(f\"Test samples: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing transforms\n",
        "preprocess = Compose([\n",
        "    LoadImaged(keys=[\"image\", \"label\"]),\n",
        "    EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "    Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "    Spacingd(\n",
        "        keys=[\"image\", \"label\"],\n",
        "        pixdim=(1.0, 1.0, 1.0),\n",
        "        mode=(\"bilinear\", \"nearest\"),\n",
        "    ),\n",
        "    ScaleIntensityRanged(\n",
        "        keys=[\"image\"],\n",
        "        a_min=-175,\n",
        "        a_max=250,\n",
        "        b_min=0.0,\n",
        "        b_max=1.0,\n",
        "        clip=True,\n",
        "    ),\n",
        "    CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "    EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 3D Volume Inference Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_volume(model, volume, target_size=224, device='cpu'):\n",
        "    \"\"\"\n",
        "    Run inference on a 3D volume by slicing and stacking.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained segmentation model\n",
        "        volume: 3D numpy array (D, H, W) or (C, D, H, W)\n",
        "        target_size: Size to resize slices to\n",
        "        device: Torch device\n",
        "        \n",
        "    Returns:\n",
        "        3D prediction array (D, H, W)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Handle channel dimension\n",
        "    if volume.ndim == 4:\n",
        "        volume = volume[0]\n",
        "    \n",
        "    # Convert to numpy if tensor\n",
        "    if torch.is_tensor(volume):\n",
        "        volume = volume.numpy()\n",
        "    \n",
        "    D, H, W = volume.shape\n",
        "    predictions = np.zeros((D, H, W), dtype=np.int64)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for slice_idx in range(D):\n",
        "            # Get slice\n",
        "            slice_2d = volume[slice_idx, :, :]\n",
        "            \n",
        "            # Resize to target size\n",
        "            zoom_factors = (target_size / H, target_size / W)\n",
        "            slice_resized = zoom(slice_2d, zoom_factors, order=1)\n",
        "            \n",
        "            # Convert to tensor\n",
        "            slice_tensor = torch.from_numpy(slice_resized).float()\n",
        "            slice_tensor = slice_tensor.unsqueeze(0).unsqueeze(0)  # (1, 1, H, W)\n",
        "            slice_tensor = slice_tensor.to(device)\n",
        "            \n",
        "            # Predict\n",
        "            output = model(slice_tensor)\n",
        "            pred = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n",
        "            \n",
        "            # Resize back to original size\n",
        "            zoom_factors_back = (H / target_size, W / target_size)\n",
        "            pred_resized = zoom(pred.astype(float), zoom_factors_back, order=0)\n",
        "            \n",
        "            predictions[slice_idx] = pred_resized.astype(np.int64)\n",
        "    \n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_volume(prediction, ground_truth):\n",
        "    \"\"\"\n",
        "    Calculate Dice Score and Hausdorff Distance for a volume.\n",
        "    \n",
        "    Args:\n",
        "        prediction: 3D prediction array\n",
        "        ground_truth: 3D ground truth array\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with metrics\n",
        "    \"\"\"\n",
        "    # Binarize (merge tumor with pancreas)\n",
        "    pred_binary = (prediction > 0).astype(np.float32)\n",
        "    gt_binary = (ground_truth > 0).astype(np.float32)\n",
        "    \n",
        "    # Dice Score\n",
        "    intersection = np.sum(pred_binary * gt_binary)\n",
        "    union = np.sum(pred_binary) + np.sum(gt_binary)\n",
        "    \n",
        "    if union == 0:\n",
        "        dice = 1.0 if np.sum(pred_binary) == 0 else 0.0\n",
        "    else:\n",
        "        dice = 2.0 * intersection / union\n",
        "    \n",
        "    # Hausdorff Distance using MONAI\n",
        "    pred_tensor = torch.from_numpy(pred_binary).unsqueeze(0).unsqueeze(0)\n",
        "    gt_tensor = torch.from_numpy(gt_binary).unsqueeze(0).unsqueeze(0)\n",
        "    \n",
        "    hd_metric = HausdorffDistanceMetric(include_background=False, percentile=95)\n",
        "    \n",
        "    try:\n",
        "        hd_metric(y_pred=pred_tensor, y=gt_tensor)\n",
        "        hd = hd_metric.aggregate().item()\n",
        "    except:\n",
        "        hd = float('inf')  # If one of the masks is empty\n",
        "    \n",
        "    return {\n",
        "        \"dice\": dice,\n",
        "        \"hausdorff_95\": hd,\n",
        "        \"pred_volume\": np.sum(pred_binary),\n",
        "        \"gt_volume\": np.sum(gt_binary),\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Run Evaluation on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "results = []\n",
        "predictions_list = []\n",
        "ground_truths_list = []\n",
        "images_list = []\n",
        "\n",
        "print(\"Evaluating on test set...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, sample in enumerate(tqdm(test_data[:5], desc=\"Processing volumes\")):  # Limit for demo\n",
        "    # Load and preprocess\n",
        "    data = preprocess(sample)\n",
        "    image = data[\"image\"]\n",
        "    label = data[\"label\"]\n",
        "    \n",
        "    # Handle dimensions\n",
        "    if image.ndim == 4:\n",
        "        image = image[0]\n",
        "    if label.ndim == 4:\n",
        "        label = label[0]\n",
        "    \n",
        "    # Convert to numpy\n",
        "    if torch.is_tensor(image):\n",
        "        image = image.numpy()\n",
        "    if torch.is_tensor(label):\n",
        "        label = label.numpy()\n",
        "    \n",
        "    # Predict\n",
        "    prediction = predict_volume(model, image, target_size=IMG_SIZE, device=device)\n",
        "    \n",
        "    # Evaluate\n",
        "    metrics = evaluate_volume(prediction, label)\n",
        "    metrics[\"sample_id\"] = i\n",
        "    results.append(metrics)\n",
        "    \n",
        "    # Store for visualization\n",
        "    predictions_list.append(prediction)\n",
        "    ground_truths_list.append(label)\n",
        "    images_list.append(image)\n",
        "    \n",
        "    print(f\"Sample {i+1}: Dice={metrics['dice']:.4f}, HD95={metrics['hausdorff_95']:.2f}mm\")\n",
        "\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "dice_scores = [r[\"dice\"] for r in results]\n",
        "hd_scores = [r[\"hausdorff_95\"] for r in results if r[\"hausdorff_95\"] != float('inf')]\n",
        "\n",
        "print(\"\\nTest Set Performance Summary:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Dice Score:\")\n",
        "print(f\"  Mean: {np.mean(dice_scores):.4f}\")\n",
        "print(f\"  Std:  {np.std(dice_scores):.4f}\")\n",
        "print(f\"  Min:  {np.min(dice_scores):.4f}\")\n",
        "print(f\"  Max:  {np.max(dice_scores):.4f}\")\n",
        "\n",
        "if hd_scores:\n",
        "    print(f\"\\nHausdorff Distance (95th percentile):\")\n",
        "    print(f\"  Mean: {np.mean(hd_scores):.2f} mm\")\n",
        "    print(f\"  Std:  {np.std(hd_scores):.2f} mm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Publication-Ready Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_comparison_figure(image, ground_truth, prediction, patient_idx=0, slice_idx=None):\n",
        "    \"\"\"\n",
        "    Create a publication-ready comparison figure.\n",
        "    \n",
        "    Args:\n",
        "        image: 3D image array\n",
        "        ground_truth: 3D ground truth array\n",
        "        prediction: 3D prediction array\n",
        "        patient_idx: Patient index for title\n",
        "        slice_idx: Optional specific slice index\n",
        "    \"\"\"\n",
        "    # Find optimal slice (max pancreas area)\n",
        "    if slice_idx is None:\n",
        "        gt_area = np.sum(ground_truth > 0, axis=(1, 2))\n",
        "        slice_idx = np.argmax(gt_area)\n",
        "    \n",
        "    # Extract slices\n",
        "    img_slice = image[slice_idx]\n",
        "    gt_slice = ground_truth[slice_idx]\n",
        "    pred_slice = prediction[slice_idx]\n",
        "    \n",
        "    # Create figure\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "    \n",
        "    # 1. Original CT\n",
        "    axes[0].imshow(img_slice, cmap=\"gray\")\n",
        "    axes[0].set_title(\"Original CT\", fontsize=14, fontweight=\"bold\")\n",
        "    axes[0].axis(\"off\")\n",
        "    \n",
        "    # 2. Ground Truth (Green)\n",
        "    axes[1].imshow(img_slice, cmap=\"gray\")\n",
        "    gt_mask = np.ma.masked_where(gt_slice == 0, gt_slice)\n",
        "    axes[1].imshow(gt_mask, cmap=\"Greens\", alpha=0.7, vmin=0, vmax=2)\n",
        "    axes[1].set_title(\"Ground Truth\", fontsize=14, fontweight=\"bold\")\n",
        "    axes[1].axis(\"off\")\n",
        "    \n",
        "    # 3. Prediction (Red)\n",
        "    axes[2].imshow(img_slice, cmap=\"gray\")\n",
        "    pred_mask = np.ma.masked_where(pred_slice == 0, pred_slice)\n",
        "    axes[2].imshow(pred_mask, cmap=\"Reds\", alpha=0.7, vmin=0, vmax=2)\n",
        "    axes[2].set_title(\"Model Prediction\", fontsize=14, fontweight=\"bold\")\n",
        "    axes[2].axis(\"off\")\n",
        "    \n",
        "    # 4. Overlay comparison\n",
        "    axes[3].imshow(img_slice, cmap=\"gray\")\n",
        "    \n",
        "    # True Positive (Green), False Positive (Red), False Negative (Blue)\n",
        "    gt_binary = gt_slice > 0\n",
        "    pred_binary = pred_slice > 0\n",
        "    \n",
        "    tp = gt_binary & pred_binary\n",
        "    fp = ~gt_binary & pred_binary\n",
        "    fn = gt_binary & ~pred_binary\n",
        "    \n",
        "    overlay = np.zeros((*img_slice.shape, 4))\n",
        "    overlay[tp] = [0, 1, 0, 0.5]  # Green\n",
        "    overlay[fp] = [1, 0, 0, 0.5]  # Red\n",
        "    overlay[fn] = [0, 0, 1, 0.5]  # Blue\n",
        "    \n",
        "    axes[3].imshow(overlay)\n",
        "    axes[3].set_title(\"Comparison\\n(TP: Green, FP: Red, FN: Blue)\", fontsize=14, fontweight=\"bold\")\n",
        "    axes[3].axis(\"off\")\n",
        "    \n",
        "    # Calculate metrics for this slice\n",
        "    dice = 2 * np.sum(tp) / (np.sum(gt_binary) + np.sum(pred_binary) + 1e-8)\n",
        "    \n",
        "    plt.suptitle(\n",
        "        f\"Patient {patient_idx + 1} - Slice {slice_idx} | Dice Score: {dice:.4f}\",\n",
        "        fontsize=16, fontweight=\"bold\", y=1.02\n",
        "    )\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comparison figures for all test samples\n",
        "print(\"Generating comparison figures...\")\n",
        "\n",
        "for i in range(len(predictions_list)):\n",
        "    fig = create_comparison_figure(\n",
        "        images_list[i],\n",
        "        ground_truths_list[i],\n",
        "        predictions_list[i],\n",
        "        patient_idx=i\n",
        "    )\n",
        "    \n",
        "    # Save figure\n",
        "    fig.savefig(\n",
        "        OUTPUT_DIR / f\"comparison_patient_{i+1}.png\",\n",
        "        dpi=150,\n",
        "        bbox_inches=\"tight\",\n",
        "        facecolor=\"white\"\n",
        "    )\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_multi_slice_figure(image, ground_truth, prediction, patient_idx=0, num_slices=6):\n",
        "    \"\"\"\n",
        "    Create a figure showing multiple slices.\n",
        "    \"\"\"\n",
        "    # Find slices with pancreas\n",
        "    gt_area = np.sum(ground_truth > 0, axis=(1, 2))\n",
        "    pancreas_slices = np.where(gt_area > 0)[0]\n",
        "    \n",
        "    if len(pancreas_slices) < num_slices:\n",
        "        selected = pancreas_slices\n",
        "    else:\n",
        "        indices = np.linspace(0, len(pancreas_slices)-1, num_slices, dtype=int)\n",
        "        selected = pancreas_slices[indices]\n",
        "    \n",
        "    fig, axes = plt.subplots(3, len(selected), figsize=(3*len(selected), 9))\n",
        "    \n",
        "    for col, slice_idx in enumerate(selected):\n",
        "        img_slice = image[slice_idx]\n",
        "        gt_slice = ground_truth[slice_idx]\n",
        "        pred_slice = prediction[slice_idx]\n",
        "        \n",
        "        # Row 0: CT\n",
        "        axes[0, col].imshow(img_slice, cmap=\"gray\")\n",
        "        axes[0, col].set_title(f\"Slice {slice_idx}\", fontsize=10)\n",
        "        axes[0, col].axis(\"off\")\n",
        "        \n",
        "        # Row 1: Ground Truth\n",
        "        axes[1, col].imshow(img_slice, cmap=\"gray\")\n",
        "        gt_mask = np.ma.masked_where(gt_slice == 0, gt_slice)\n",
        "        axes[1, col].imshow(gt_mask, cmap=\"Greens\", alpha=0.7, vmin=0, vmax=2)\n",
        "        axes[1, col].axis(\"off\")\n",
        "        \n",
        "        # Row 2: Prediction\n",
        "        axes[2, col].imshow(img_slice, cmap=\"gray\")\n",
        "        pred_mask = np.ma.masked_where(pred_slice == 0, pred_slice)\n",
        "        axes[2, col].imshow(pred_mask, cmap=\"Reds\", alpha=0.7, vmin=0, vmax=2)\n",
        "        axes[2, col].axis(\"off\")\n",
        "    \n",
        "    # Row labels\n",
        "    axes[0, 0].set_ylabel(\"CT Image\", fontsize=12, fontweight=\"bold\", rotation=0, ha=\"right\", va=\"center\")\n",
        "    axes[1, 0].set_ylabel(\"Ground Truth\", fontsize=12, fontweight=\"bold\", rotation=0, ha=\"right\", va=\"center\")\n",
        "    axes[2, 0].set_ylabel(\"Prediction\", fontsize=12, fontweight=\"bold\", rotation=0, ha=\"right\", va=\"center\")\n",
        "    \n",
        "    dice = results[patient_idx][\"dice\"]\n",
        "    plt.suptitle(\n",
        "        f\"Patient {patient_idx + 1} - Multi-Slice View | Overall Dice: {dice:.4f}\",\n",
        "        fontsize=14, fontweight=\"bold\"\n",
        "    )\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# Generate multi-slice figure for first patient\n",
        "if len(predictions_list) > 0:\n",
        "    fig = create_multi_slice_figure(\n",
        "        images_list[0],\n",
        "        ground_truths_list[0],\n",
        "        predictions_list[0],\n",
        "        patient_idx=0\n",
        "    )\n",
        "    fig.savefig(OUTPUT_DIR / \"multi_slice_view.png\", dpi=150, bbox_inches=\"tight\", facecolor=\"white\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Attention Map Visualization (Bonus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_attention(model, image_slice, device='cpu'):\n",
        "    \"\"\"\n",
        "    Extract and visualize attention maps from the Transformer.\n",
        "    \n",
        "    Args:\n",
        "        model: TransUNet model\n",
        "        image_slice: 2D image slice (H, W)\n",
        "        device: Torch device\n",
        "        \n",
        "    Returns:\n",
        "        Attention heatmap\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Resize and prepare input\n",
        "    from scipy.ndimage import zoom\n",
        "    \n",
        "    H, W = image_slice.shape\n",
        "    zoom_factors = (IMG_SIZE / H, IMG_SIZE / W)\n",
        "    slice_resized = zoom(image_slice, zoom_factors, order=1)\n",
        "    \n",
        "    input_tensor = torch.from_numpy(slice_resized).float()\n",
        "    input_tensor = input_tensor.unsqueeze(0).unsqueeze(0).to(device)\n",
        "    \n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "    \n",
        "    # Get attention maps\n",
        "    attention_maps = model.get_attention_maps()\n",
        "    \n",
        "    if attention_maps and attention_maps[0] is not None:\n",
        "        # Use attention from last layer\n",
        "        attn = attention_maps[-1]  # (B, num_heads, N, N)\n",
        "        \n",
        "        # Average over heads\n",
        "        attn = attn.mean(dim=1)  # (B, N, N)\n",
        "        \n",
        "        # Get attention from CLS token or average\n",
        "        attn = attn[0].mean(dim=0)  # (N,)\n",
        "        \n",
        "        # Reshape to spatial\n",
        "        feature_size = int(np.sqrt(attn.shape[0]))\n",
        "        attn_map = attn.cpu().numpy().reshape(feature_size, feature_size)\n",
        "        \n",
        "        # Resize to input size\n",
        "        zoom_back = (IMG_SIZE / feature_size, IMG_SIZE / feature_size)\n",
        "        attn_map = zoom(attn_map, zoom_back, order=1)\n",
        "        \n",
        "        return attn_map, slice_resized, output\n",
        "    else:\n",
        "        return None, slice_resized, output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize attention for a sample\n",
        "if len(images_list) > 0:\n",
        "    # Get a slice with good pancreas coverage\n",
        "    gt_area = np.sum(ground_truths_list[0] > 0, axis=(1, 2))\n",
        "    best_slice = np.argmax(gt_area)\n",
        "    \n",
        "    image_slice = images_list[0][best_slice]\n",
        "    \n",
        "    attn_map, resized_img, output = visualize_attention(model, image_slice, device)\n",
        "    \n",
        "    if attn_map is not None:\n",
        "        fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "        \n",
        "        # Original image\n",
        "        axes[0].imshow(resized_img, cmap=\"gray\")\n",
        "        axes[0].set_title(\"Input Image\", fontsize=14, fontweight=\"bold\")\n",
        "        axes[0].axis(\"off\")\n",
        "        \n",
        "        # Attention map\n",
        "        im = axes[1].imshow(attn_map, cmap=\"hot\")\n",
        "        axes[1].set_title(\"Attention Map\", fontsize=14, fontweight=\"bold\")\n",
        "        axes[1].axis(\"off\")\n",
        "        plt.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)\n",
        "        \n",
        "        # Attention overlay\n",
        "        axes[2].imshow(resized_img, cmap=\"gray\")\n",
        "        axes[2].imshow(attn_map, cmap=\"hot\", alpha=0.5)\n",
        "        axes[2].set_title(\"Attention Overlay\", fontsize=14, fontweight=\"bold\")\n",
        "        axes[2].axis(\"off\")\n",
        "        \n",
        "        # Prediction\n",
        "        pred = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n",
        "        axes[3].imshow(resized_img, cmap=\"gray\")\n",
        "        pred_mask = np.ma.masked_where(pred == 0, pred)\n",
        "        axes[3].imshow(pred_mask, cmap=\"Reds\", alpha=0.7)\n",
        "        axes[3].set_title(\"Segmentation Output\", fontsize=14, fontweight=\"bold\")\n",
        "        axes[3].axis(\"off\")\n",
        "        \n",
        "        plt.suptitle(\"Transformer Attention Visualization\", fontsize=16, fontweight=\"bold\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(OUTPUT_DIR / \"attention_visualization.png\", dpi=150, bbox_inches=\"tight\")\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Attention maps not available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save evaluation results\n",
        "results_summary = {\n",
        "    \"num_samples\": len(results),\n",
        "    \"dice_mean\": float(np.mean(dice_scores)),\n",
        "    \"dice_std\": float(np.std(dice_scores)),\n",
        "    \"dice_min\": float(np.min(dice_scores)),\n",
        "    \"dice_max\": float(np.max(dice_scores)),\n",
        "    \"individual_results\": results,\n",
        "}\n",
        "\n",
        "if hd_scores:\n",
        "    results_summary[\"hd95_mean\"] = float(np.mean(hd_scores))\n",
        "    results_summary[\"hd95_std\"] = float(np.std(hd_scores))\n",
        "\n",
        "# Save to JSON\n",
        "results_path = OUTPUT_DIR / \"evaluation_results.json\"\n",
        "with open(results_path, \"w\") as f:\n",
        "    json.dump(results_summary, f, indent=2)\n",
        "\n",
        "print(f\"Results saved to: {results_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook completed the evaluation pipeline:\n",
        "\n",
        "1. **Model Loading**: Loaded trained TransUNet from checkpoint\n",
        "\n",
        "2. **3D Volume Inference**:\n",
        "   - Slice-by-slice prediction\n",
        "   - Resize to model input size\n",
        "   - Stack predictions back to 3D\n",
        "\n",
        "3. **Metrics**:\n",
        "   - Dice Score: Overlap measure\n",
        "   - Hausdorff Distance (95th percentile): Surface distance measure\n",
        "\n",
        "4. **Visualizations**:\n",
        "   - Side-by-side comparison (CT, Ground Truth, Prediction)\n",
        "   - Error analysis (TP, FP, FN overlay)\n",
        "   - Multi-slice view\n",
        "\n",
        "5. **Attention Maps**:\n",
        "   - Extracted from Transformer bottleneck\n",
        "   - Overlaid on input to show model focus areas\n",
        "\n",
        "**Outputs**:\n",
        "- `outputs/evaluation_results.json`: Quantitative metrics\n",
        "- `outputs/comparison_patient_*.png`: Per-patient visualizations\n",
        "- `outputs/multi_slice_view.png`: Multi-slice overview\n",
        "- `outputs/attention_visualization.png`: Attention map visualization"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
