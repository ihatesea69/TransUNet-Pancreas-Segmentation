{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 02: Model Architecture (TransUNet)\n",
        "\n",
        "## Pancreas CT Segmentation using TransUNet\n",
        "\n",
        "This notebook covers:\n",
        "1. TransUNet architecture overview\n",
        "2. Component implementation details\n",
        "3. Model instantiation and testing\n",
        "4. Parameter count analysis\n",
        "5. Forward pass verification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Architecture Overview\n",
        "\n",
        "TransUNet combines the strengths of:\n",
        "- **CNN Encoder**: Local feature extraction with spatial hierarchy\n",
        "- **Transformer Bottleneck**: Global context via self-attention\n",
        "- **U-Net Decoder**: Precise localization with skip connections\n",
        "\n",
        "```\n",
        "Input Image (224x224)\n",
        "       │\n",
        "       ▼\n",
        "┌─────────────────┐\n",
        "│   CNN Encoder   │  ← ResNet-style backbone\n",
        "│  (Multi-scale)  │\n",
        "└────────┬────────┘\n",
        "         │ Skip connections\n",
        "         ▼\n",
        "┌─────────────────┐\n",
        "│   Transformer   │  ← ViT blocks with self-attention\n",
        "│   Bottleneck    │\n",
        "└────────┬────────┘\n",
        "         │\n",
        "         ▼\n",
        "┌─────────────────┐\n",
        "│   CNN Decoder   │  ← U-Net style upsampling\n",
        "│  (Skip + Up)    │\n",
        "└────────┬────────┘\n",
        "         │\n",
        "         ▼\n",
        "  Segmentation Map\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import sys\n",
        "import math\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, str(Path.cwd()))\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Transformer Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Head Self-Attention mechanism.\n",
        "    \n",
        "    Computes attention as: Attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) * V\n",
        "    Multiple heads allow attending to different representation subspaces.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        \n",
        "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "        \n",
        "        # Combined QKV projection\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        # Store attention weights for visualization\n",
        "        self.attention_weights = None\n",
        "    \n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        \n",
        "        # Compute Q, K, V\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, num_heads, N, head_dim)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        \n",
        "        # Scaled dot-product attention\n",
        "        scale = self.head_dim ** -0.5\n",
        "        attn = (q @ k.transpose(-2, -1)) * scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "        \n",
        "        # Store for visualization\n",
        "        self.attention_weights = attn.detach()\n",
        "        \n",
        "        # Apply attention to values\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Test Multi-Head Attention\n",
        "print(\"Testing MultiHeadAttention...\")\n",
        "mha = MultiHeadAttention(embed_dim=768, num_heads=12)\n",
        "x = torch.randn(2, 196, 768)  # (batch, num_patches, embed_dim)\n",
        "out = mha(x)\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {out.shape}\")\n",
        "print(f\"Attention shape: {mha.attention_weights.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    MLP block with GELU activation.\n",
        "    \n",
        "    Standard feedforward network: Linear -> GELU -> Dropout -> Linear -> Dropout\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, dropout=0.0):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features * 4\n",
        "        \n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Single Transformer encoder block.\n",
        "    \n",
        "    Structure:\n",
        "        x -> LayerNorm -> MultiHeadAttention -> Add (residual)\n",
        "          -> LayerNorm -> MLP -> Add (residual)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.0):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = MLP(embed_dim, int(embed_dim * mlp_ratio), dropout=dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "# Test Transformer Block\n",
        "print(\"Testing TransformerBlock...\")\n",
        "block = TransformerBlock(embed_dim=768, num_heads=12)\n",
        "x = torch.randn(2, 196, 768)\n",
        "out = block(x)\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {out.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Vision Transformer (ViT) Bottleneck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer (ViT) for processing CNN feature maps.\n",
        "    \n",
        "    Takes flattened feature patches as input and applies self-attention\n",
        "    to capture global context across the entire feature map.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim=768,\n",
        "        num_heads=12,\n",
        "        num_layers=12,\n",
        "        mlp_ratio=4.0,\n",
        "        dropout=0.0,\n",
        "        num_patches=196,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_patches = num_patches\n",
        "        \n",
        "        # Learnable positional embeddings\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
        "        self.pos_dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        # Transformer blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        \n",
        "        # Initialize positional embeddings\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x: (B, N, embed_dim) where N is number of patches\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_dropout(x)\n",
        "        \n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        \n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "    \n",
        "    def get_attention_maps(self):\n",
        "        \"\"\"Return attention weights from all layers.\"\"\"\n",
        "        return [block.attn.attention_weights for block in self.blocks]\n",
        "\n",
        "# Test Vision Transformer\n",
        "print(\"Testing VisionTransformer...\")\n",
        "vit = VisionTransformer(\n",
        "    embed_dim=768,\n",
        "    num_heads=12,\n",
        "    num_layers=6,  # Use fewer layers for testing\n",
        "    num_patches=49,  # 7x7 feature map\n",
        ")\n",
        "x = torch.randn(2, 49, 768)\n",
        "out = vit(x)\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {out.shape}\")\n",
        "print(f\"Number of attention maps: {len(vit.get_attention_maps())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. CNN Encoder (ResNet-style)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Residual block with skip connection.\"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = downsample\n",
        "    \n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        \n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        \n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        \n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "        \n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "class CNNEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN Encoder extracting multi-scale features.\n",
        "    \n",
        "    Returns features at 4 scales: 1/4, 1/8, 1/16, 1/32 of input resolution.\n",
        "    These are used as skip connections for the decoder.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels=1, base_channels=64):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Initial convolution: reduces resolution by 4x\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, base_channels, 7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(base_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "        )\n",
        "        \n",
        "        # Encoder stages\n",
        "        self.layer1 = self._make_layer(base_channels, base_channels, 3)\n",
        "        self.layer2 = self._make_layer(base_channels, base_channels * 2, 4, stride=2)\n",
        "        self.layer3 = self._make_layer(base_channels * 2, base_channels * 4, 6, stride=2)\n",
        "        self.layer4 = self._make_layer(base_channels * 4, base_channels * 8, 3, stride=2)\n",
        "        \n",
        "        # Channel dimensions for skip connections\n",
        "        self.skip_channels = [base_channels, base_channels * 2, base_channels * 4, base_channels * 8]\n",
        "    \n",
        "    def _make_layer(self, in_channels, out_channels, num_blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "            )\n",
        "        \n",
        "        layers = [ResidualBlock(in_channels, out_channels, stride, downsample)]\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(ResidualBlock(out_channels, out_channels))\n",
        "        \n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Returns skip connections at each scale\n",
        "        x = self.stem(x)  # 1/4\n",
        "        \n",
        "        skip1 = self.layer1(x)   # 1/4, 64 channels\n",
        "        skip2 = self.layer2(skip1)  # 1/8, 128 channels\n",
        "        skip3 = self.layer3(skip2)  # 1/16, 256 channels\n",
        "        skip4 = self.layer4(skip3)  # 1/32, 512 channels\n",
        "        \n",
        "        return [skip1, skip2, skip3, skip4]\n",
        "\n",
        "# Test CNN Encoder\n",
        "print(\"Testing CNNEncoder...\")\n",
        "encoder = CNNEncoder(in_channels=1, base_channels=64)\n",
        "x = torch.randn(2, 1, 224, 224)\n",
        "skips = encoder(x)\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "for i, skip in enumerate(skips):\n",
        "    print(f\"Skip {i+1} shape: {skip.shape} (1/{2**(i+2)} resolution)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. CNN Decoder (U-Net style)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"Decoder block with upsampling and skip connection.\"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels, skip_channels, out_channels):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels // 2 + skip_channels, out_channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x, skip):\n",
        "        x = self.up(x)\n",
        "        \n",
        "        # Handle size mismatch\n",
        "        if x.shape != skip.shape:\n",
        "            x = F.interpolate(x, size=skip.shape[2:], mode=\"bilinear\", align_corners=True)\n",
        "        \n",
        "        x = torch.cat([x, skip], dim=1)\n",
        "        x = self.conv(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class CNNDecoder(nn.Module):\n",
        "    \"\"\"U-Net style decoder with skip connections from encoder.\"\"\"\n",
        "    \n",
        "    def __init__(self, encoder_channels, embed_dim=768, base_channels=64):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Project transformer output back to spatial features\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Conv2d(embed_dim, encoder_channels[-1], 1),\n",
        "            nn.BatchNorm2d(encoder_channels[-1]),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        \n",
        "        # Decoder blocks (from deepest to shallowest)\n",
        "        self.decoder4 = DecoderBlock(encoder_channels[-1], encoder_channels[-2], base_channels * 4)\n",
        "        self.decoder3 = DecoderBlock(base_channels * 4, encoder_channels[-3], base_channels * 2)\n",
        "        self.decoder2 = DecoderBlock(base_channels * 2, encoder_channels[-4], base_channels)\n",
        "        self.decoder1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(base_channels, base_channels, kernel_size=2, stride=2),\n",
        "            nn.Conv2d(base_channels, base_channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(base_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        \n",
        "        # Final upsampling to match input resolution\n",
        "        self.final_up = nn.ConvTranspose2d(base_channels, base_channels, kernel_size=2, stride=2)\n",
        "    \n",
        "    def forward(self, x, skips, feature_size):\n",
        "        B, N, C = x.shape\n",
        "        H, W = feature_size\n",
        "        \n",
        "        # Reshape transformer output to spatial\n",
        "        x = x.transpose(1, 2).reshape(B, C, H, W)\n",
        "        x = self.proj(x)\n",
        "        \n",
        "        # Decoder with skip connections\n",
        "        x = self.decoder4(x, skips[2])  # 1/16 -> 1/8\n",
        "        x = self.decoder3(x, skips[1])  # 1/8 -> 1/4\n",
        "        x = self.decoder2(x, skips[0])  # 1/4 -> 1/2\n",
        "        x = self.decoder1(x)            # 1/2 -> 1/1\n",
        "        x = self.final_up(x)            # Final upsampling\n",
        "        \n",
        "        return x\n",
        "\n",
        "print(\"Decoder components defined successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Complete TransUNet Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransUNet(nn.Module):\n",
        "    \"\"\"\n",
        "    TransUNet: Transformer + U-Net for Medical Image Segmentation.\n",
        "    \n",
        "    Architecture:\n",
        "        Input -> CNN Encoder -> Transformer Bottleneck -> CNN Decoder -> Output\n",
        "    \n",
        "    Args:\n",
        "        img_size: Input image size (default: 224)\n",
        "        in_channels: Number of input channels (default: 1 for CT)\n",
        "        out_channels: Number of output classes (default: 2)\n",
        "        embed_dim: Transformer embedding dimension (default: 768)\n",
        "        num_heads: Number of attention heads (default: 12)\n",
        "        num_layers: Number of transformer layers (default: 12)\n",
        "        mlp_ratio: MLP hidden dimension ratio (default: 4.0)\n",
        "        dropout: Dropout rate (default: 0.1)\n",
        "        base_channels: Base channel count for CNN (default: 64)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=224,\n",
        "        in_channels=1,\n",
        "        out_channels=2,\n",
        "        embed_dim=768,\n",
        "        num_heads=12,\n",
        "        num_layers=12,\n",
        "        mlp_ratio=4.0,\n",
        "        dropout=0.1,\n",
        "        base_channels=64,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.img_size = img_size\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.embed_dim = embed_dim\n",
        "        \n",
        "        # CNN Encoder\n",
        "        self.encoder = CNNEncoder(in_channels, base_channels)\n",
        "        encoder_out_channels = self.encoder.skip_channels[-1]  # 512\n",
        "        \n",
        "        # Calculate feature map size after encoder (1/32 of input)\n",
        "        self.feature_size = img_size // 32\n",
        "        self.num_patches = self.feature_size ** 2\n",
        "        \n",
        "        # Patch embedding: project CNN features to transformer dimension\n",
        "        self.patch_embed = nn.Sequential(\n",
        "            nn.Conv2d(encoder_out_channels, embed_dim, kernel_size=1),\n",
        "            nn.Flatten(2),  # (B, embed_dim, H*W)\n",
        "        )\n",
        "        \n",
        "        # Vision Transformer\n",
        "        self.transformer = VisionTransformer(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            num_layers=num_layers,\n",
        "            mlp_ratio=mlp_ratio,\n",
        "            dropout=dropout,\n",
        "            num_patches=self.num_patches,\n",
        "        )\n",
        "        \n",
        "        # CNN Decoder\n",
        "        self.decoder = CNNDecoder(\n",
        "            encoder_channels=self.encoder.skip_channels,\n",
        "            embed_dim=embed_dim,\n",
        "            base_channels=base_channels,\n",
        "        )\n",
        "        \n",
        "        # Segmentation head\n",
        "        self.seg_head = nn.Conv2d(base_channels, out_channels, kernel_size=1)\n",
        "        \n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "        \n",
        "        print(f\"TransUNet initialized:\")\n",
        "        print(f\"  Input: {in_channels}x{img_size}x{img_size}\")\n",
        "        print(f\"  Output: {out_channels}x{img_size}x{img_size}\")\n",
        "        print(f\"  Feature map size: {self.feature_size}x{self.feature_size}\")\n",
        "        print(f\"  Num patches: {self.num_patches}\")\n",
        "        print(f\"  Transformer: {num_layers} layers, {num_heads} heads, {embed_dim} dim\")\n",
        "    \n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        \n",
        "        # CNN Encoder: extract multi-scale features\n",
        "        skips = self.encoder(x)  # [skip1, skip2, skip3, skip4]\n",
        "        \n",
        "        # Patch embedding: project deepest features to transformer dimension\n",
        "        x = self.patch_embed(skips[-1])  # (B, embed_dim, N)\n",
        "        x = x.transpose(1, 2)  # (B, N, embed_dim)\n",
        "        \n",
        "        # Transformer: global context modeling\n",
        "        x = self.transformer(x)\n",
        "        \n",
        "        # CNN Decoder: upsample with skip connections\n",
        "        feature_size = (self.feature_size, self.feature_size)\n",
        "        x = self.decoder(x, skips, feature_size)\n",
        "        \n",
        "        # Segmentation head\n",
        "        x = self.seg_head(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def get_attention_maps(self):\n",
        "        \"\"\"Get attention maps from transformer for visualization.\"\"\"\n",
        "        return self.transformer.get_attention_maps()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and test model\n",
        "print(\"Creating TransUNet model...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model = TransUNet(\n",
        "    img_size=224,\n",
        "    in_channels=1,\n",
        "    out_channels=2,\n",
        "    embed_dim=768,\n",
        "    num_heads=12,\n",
        "    num_layers=12,\n",
        "    base_channels=64,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test forward pass\n",
        "print(\"\\nTesting forward pass...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create dummy input\n",
        "batch_size = 2\n",
        "x = torch.randn(batch_size, 1, 224, 224)\n",
        "\n",
        "# Forward pass\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(x)\n",
        "\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Expected:     ({batch_size}, 2, 224, 224)\")\n",
        "print(f\"\\nShape match: {output.shape == (batch_size, 2, 224, 224)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count parameters\n",
        "def count_parameters(model):\n",
        "    \"\"\"Count trainable and total parameters.\"\"\"\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return total, trainable\n",
        "\n",
        "total_params, trainable_params = count_parameters(model)\n",
        "\n",
        "print(\"\\nParameter Count:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total parameters:     {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Model size:           {total_params * 4 / 1e6:.1f} MB (float32)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed parameter breakdown\n",
        "def parameter_breakdown(model):\n",
        "    \"\"\"Show parameter count per major component.\"\"\"\n",
        "    breakdown = {}\n",
        "    \n",
        "    for name, module in model.named_children():\n",
        "        params = sum(p.numel() for p in module.parameters())\n",
        "        breakdown[name] = params\n",
        "    \n",
        "    return breakdown\n",
        "\n",
        "breakdown = parameter_breakdown(model)\n",
        "total = sum(breakdown.values())\n",
        "\n",
        "print(\"\\nParameter Breakdown:\")\n",
        "print(\"=\"*60)\n",
        "for name, count in breakdown.items():\n",
        "    percentage = 100 * count / total\n",
        "    print(f\"{name:20s}: {count:>12,} ({percentage:5.1f}%)\")\n",
        "print(\"-\"*60)\n",
        "print(f\"{'Total':20s}: {total:>12,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize a sample output\n",
        "print(\"\\nVisualizing sample output...\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "# Input\n",
        "axes[0].imshow(x[0, 0].numpy(), cmap=\"gray\")\n",
        "axes[0].set_title(\"Input Image\")\n",
        "axes[0].axis(\"off\")\n",
        "\n",
        "# Output class 0 (background)\n",
        "axes[1].imshow(output[0, 0].detach().numpy(), cmap=\"viridis\")\n",
        "axes[1].set_title(\"Output: Background Logits\")\n",
        "axes[1].axis(\"off\")\n",
        "\n",
        "# Output class 1 (pancreas)\n",
        "axes[2].imshow(output[0, 1].detach().numpy(), cmap=\"viridis\")\n",
        "axes[2].set_title(\"Output: Pancreas Logits\")\n",
        "axes[2].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Load Model from Source Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import from src module\n",
        "from src.model import TransUNet, create_transunet\n",
        "\n",
        "# Create model using factory function\n",
        "print(\"Creating model using factory function...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Small variant (for testing)\n",
        "model_small = create_transunet(\n",
        "    img_size=224,\n",
        "    in_channels=1,\n",
        "    out_channels=2,\n",
        "    variant=\"small\",\n",
        ")\n",
        "\n",
        "print(f\"\\nSmall variant parameters: {sum(p.numel() for p in model_small.parameters()):,}\")\n",
        "\n",
        "# Base variant (default)\n",
        "model_base = create_transunet(\n",
        "    img_size=224,\n",
        "    in_channels=1,\n",
        "    out_channels=2,\n",
        "    variant=\"base\",\n",
        ")\n",
        "\n",
        "print(f\"Base variant parameters: {sum(p.numel() for p in model_base.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify model works with actual data shape\n",
        "print(\"\\nFinal verification with base model...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "x = torch.randn(1, 1, 224, 224)\n",
        "model_base.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    y = model_base(x)\n",
        "\n",
        "print(f\"Input:  {x.shape}\")\n",
        "print(f\"Output: {y.shape}\")\n",
        "print(f\"\\nModel ready for training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook implemented and verified the TransUNet architecture:\n",
        "\n",
        "1. **Transformer Components**:\n",
        "   - Multi-Head Self-Attention with stored attention weights\n",
        "   - MLP block with GELU activation\n",
        "   - Transformer block with pre-norm residual connections\n",
        "\n",
        "2. **CNN Encoder**:\n",
        "   - ResNet-style backbone with residual blocks\n",
        "   - Multi-scale feature extraction (1/4, 1/8, 1/16, 1/32)\n",
        "   - Skip connections for decoder\n",
        "\n",
        "3. **Vision Transformer Bottleneck**:\n",
        "   - Processes CNN features with self-attention\n",
        "   - Learnable positional embeddings\n",
        "   - Captures global context\n",
        "\n",
        "4. **CNN Decoder**:\n",
        "   - U-Net style upsampling with transposed convolutions\n",
        "   - Skip connections from encoder\n",
        "   - Progressive resolution recovery\n",
        "\n",
        "5. **Model Verification**:\n",
        "   - Input: (B, 1, 224, 224)\n",
        "   - Output: (B, 2, 224, 224)\n",
        "   - Parameters: ~105M (base variant)\n",
        "\n",
        "Next: Notebook 03 - Training Pipeline"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
